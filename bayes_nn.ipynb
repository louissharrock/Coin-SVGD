{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "966b5ac8",
   "metadata": {},
   "source": [
    "## Bayesian Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8f2eb4",
   "metadata": {},
   "source": [
    "In this notebook we use Coin SVGD (and SVGD) to perform inference in a Bayesian neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecefd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from plot_utils import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8d85dd",
   "metadata": {},
   "source": [
    "Before we define the model, let's define somewhere to save any results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96d3d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up directories for plots & results\n",
    "plot_dir = \"plots/SVGD/BayesNN\"\n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)\n",
    "\n",
    "results_dir = \"results/SVGD/BayesNN\"\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "time_plot_dir = plot_dir\n",
    "if not os.path.exists(time_plot_dir):\n",
    "    os.makedirs(time_plot_dir)\n",
    "\n",
    "time_results_dir = results_dir\n",
    "if not os.path.exists(time_results_dir):\n",
    "    os.makedirs(time_results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39937c41",
   "metadata": {},
   "source": [
    "Our model & implementation is based on the one in Liu & Wang (2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535e2d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Bayesian Neural Network, following Liu et al. 2016.\n",
    "    Adapted from https://github.com/dilinwang820/Stein-Variational-Gradient-Descent/\n",
    "    \n",
    "    The model is defined as follows:\n",
    "    p(y | W, X, \\gamma) = \\prod_i^N  N(y_i | f(x_i; W), \\gamma^{-1})\n",
    "    p(W | \\lambda) = \\prod_i N(w_i | 0, \\lambda^{-1})\n",
    "    p(\\gamma) = Gamma(\\gamma | a0, b0)\n",
    "    p(\\lambda) = Gamma(\\lambda | a0, b0)\n",
    "\n",
    "    The posterior distribution is then given by\n",
    "    p(W, \\gamma, \\lambda) = p(y | W, X, \\gamma) p(W | \\lambda) p(\\gamma) p(\\lambda).\n",
    "    \n",
    "    To avoid negative values of \\gamma and \\lambda, we will work with log_gamma and log_lambda.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class BayesNN:\n",
    "    \"\"\"\n",
    "        Neural network with one hidden layer.\n",
    "\n",
    "        Input\n",
    "            -- X_train: training dataset, features\n",
    "            -- y_train: training labels\n",
    "            -- batch_size: sub-sampling batch size\n",
    "            -- max_iter: maximum iterations for the training procedure\n",
    "            -- M: number of particles are used to fit the posterior distribution\n",
    "            -- n_hidden: number of hidden units\n",
    "            -- a0, b0: hyper-parameters of Gamma distribution\n",
    "            -- step_size, auto_corr: parameters of adagrad\n",
    "            -- do_svgd, do_coin_svgd: whether to do SVGD or coin SVGD or both\n",
    "            -- alpha: scaling parameter for neural net coin SVGD, default = 100\n",
    "            -- L_init: initial value of max. observed scale for Coin SVGD, default = 1e-10\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, batch_size=100, max_iter=1000, M=20, n_hidden=50, a0=1, b0=0.1,\n",
    "                 stepsize=1e-3, auto_corr=0.9, do_svgd=True, do_coin_svgd=True, alpha=100, L_init=1e-10):\n",
    "\n",
    "        self.n_hidden = n_hidden\n",
    "        self.d = X_train.shape[1]\n",
    "        self.M = M\n",
    "\n",
    "        # coin parameters\n",
    "        self.alpha = alpha\n",
    "        self.L_init = L_init\n",
    "\n",
    "        # w1: d*n_hidden; b1: n_hidden; w2 = n_hidden; b2 = 1; 2 variances\n",
    "        num_vars = self.d * n_hidden + n_hidden * 2 + 3\n",
    "        self.theta = np.zeros([self.M, num_vars])\n",
    "        self.theta_coin = np.zeros([self.M, num_vars])\n",
    "\n",
    "        # which methods to use\n",
    "        self.do_svgd = do_svgd\n",
    "        self.do_coin_svgd = do_coin_svgd\n",
    "\n",
    "        # development set\n",
    "        size_dev = min(int(np.round(0.1 * X_train.shape[0])), 500)\n",
    "        X_dev, y_dev = X_train[-size_dev:], y_train[-size_dev:]\n",
    "        X_dev_copy, y_dev_copy = X_train[-size_dev:], y_train[-size_dev:]\n",
    "        X_train, y_train = X_train[:-size_dev], y_train[:-size_dev]\n",
    "\n",
    "        # first normalise data sets\n",
    "        self.std_X_train = np.std(X_train, 0)\n",
    "        self.std_X_train[self.std_X_train == 0] = 1\n",
    "        self.mean_X_train = np.mean(X_train, 0)\n",
    "        self.mean_y_train = np.mean(y_train)\n",
    "        self.std_y_train = np.std(y_train)\n",
    "\n",
    "        # save rmses during training\n",
    "        self.svgd_rmses = np.zeros(max_iter)\n",
    "        self.coin_rmses = np.zeros(max_iter)\n",
    "\n",
    "        # save lls during training\n",
    "        self.svgd_lls = np.zeros(max_iter)\n",
    "        self.coin_lls = np.zeros(max_iter)\n",
    "\n",
    "        # define the neural net\n",
    "        X = T.matrix('X')  # covariates\n",
    "        y = T.vector('y')  # labels\n",
    "\n",
    "        w_1 = T.matrix('w_1')  # weights between input layer and hidden layer\n",
    "        b_1 = T.vector('b_1')  # bias vector of hidden layer\n",
    "        w_2 = T.vector('w_2')  # weights between hidden layer and output layer\n",
    "        b_2 = T.scalar('b_2')  # bias of output\n",
    "\n",
    "        N = T.scalar('N')  # number of observations\n",
    "\n",
    "        log_gamma = T.scalar('log_gamma')  # variances related parameters\n",
    "        log_lambda = T.scalar('log_lambda')\n",
    "\n",
    "        # prediction\n",
    "        prediction = T.dot(T.nnet.relu(T.dot(X, w_1) + b_1), w_2) + b_2\n",
    "\n",
    "        # log posterior\n",
    "        log_lik_data = -0.5 * X.shape[0] * (T.log(2 * np.pi) - log_gamma) - (T.exp(log_gamma) / 2) * T.sum(\n",
    "            T.power(prediction - y, 2))\n",
    "        log_prior_data = (a0 - 1) * log_gamma - b0 * T.exp(log_gamma) + log_gamma\n",
    "        log_prior_w = -0.5 * (num_vars - 2) * (T.log(2 * np.pi) - log_lambda) - (T.exp(log_lambda) / 2) * (\n",
    "                    (w_1 ** 2).sum() + (w_2 ** 2).sum() + (b_1 ** 2).sum() + b_2 ** 2) \\\n",
    "                      + (a0 - 1) * log_lambda - b0 * T.exp(log_lambda) + log_lambda\n",
    "\n",
    "        # sub-sample mini-batches\n",
    "        log_posterior = (log_lik_data * N / X.shape[0] + log_prior_data + log_prior_w)\n",
    "        dw_1, db_1, dw_2, db_2, d_log_gamma, d_log_lambda = T.grad(log_posterior,\n",
    "                                                                   [w_1, b_1, w_2, b_2, log_gamma, log_lambda])\n",
    "\n",
    "        # automatic gradient\n",
    "        logp_gradient = theano.function(\n",
    "            inputs=[X, y, w_1, b_1, w_2, b_2, log_gamma, log_lambda, N],\n",
    "            outputs=[dw_1, db_1, dw_2, db_2, d_log_gamma, d_log_lambda]\n",
    "        )\n",
    "\n",
    "        # prediction function\n",
    "        self.nn_predict = theano.function(inputs=[X, w_1, b_1, w_2, b_2], outputs=prediction)\n",
    "\n",
    "        # normalise\n",
    "        X_train, y_train = self.normalization(X_train, y_train)\n",
    "        N0 = X_train.shape[0]\n",
    "\n",
    "        # initialise\n",
    "        for i in range(self.M):\n",
    "            w1, b1, w2, b2, log_gamma, log_lambda = self.init_weights(a0, b0)\n",
    "            ridx = np.random.choice(range(X_train.shape[0]), np.min([X_train.shape[0], 1000]), replace=False)\n",
    "            y_hat = self.nn_predict(X_train[ridx, :], w1, b1, w2, b2)\n",
    "            log_gamma = -np.log(np.mean(np.power(y_hat - y_train[ridx], 2)))\n",
    "            self.theta[i, :] = self.pack_weights(w1, b1, w2, b2, log_gamma, log_lambda)\n",
    "        self.theta *= 2\n",
    "\n",
    "        # initialise coin\n",
    "        self.theta_coin = copy.deepcopy(self.theta)\n",
    "        self.theta_coin_init = copy.deepcopy(self.theta_coin)\n",
    "\n",
    "        # run svgd\n",
    "        if self.do_svgd:\n",
    "            grad_theta = np.zeros([self.M, num_vars])\n",
    "\n",
    "            # adagrad with momentum\n",
    "            fudge_factor = 1e-6\n",
    "            historical_grad = 0\n",
    "\n",
    "            # svgd\n",
    "            for iter in range(max_iter):\n",
    "\n",
    "                # sub-sampling\n",
    "                batch = [i % N0 for i in range(iter * batch_size, (iter + 1) * batch_size)]\n",
    "                for i in range(self.M):\n",
    "                    w1, b1, w2, b2, log_gamma, log_lambda = self.unpack_weights(self.theta[i, :])\n",
    "                    dw1, db1, dw2, db2, dlog_gamma, dlog_lambda = logp_gradient(X_train[batch, :], y_train[batch], w1, b1, w2,\n",
    "                                                                              b2, log_gamma, log_lambda, N0)\n",
    "                    grad_theta[i, :] = self.pack_weights(dw1, db1, dw2, db2, dlog_gamma, dlog_lambda)\n",
    "\n",
    "                # calculate svgd gradient\n",
    "                kxy, dxkxy = self.svgd_kernel(h=-1)\n",
    "                grad_theta = (np.matmul(kxy, grad_theta) + dxkxy) / self.M  # \\Phi(x)\n",
    "\n",
    "                # adagrad\n",
    "                if iter == 0:\n",
    "                    historical_grad = historical_grad + np.multiply(grad_theta, grad_theta)\n",
    "                else:\n",
    "                    historical_grad = auto_corr * historical_grad + (1 - auto_corr) * np.multiply(grad_theta, grad_theta)\n",
    "                adj_grad = np.divide(grad_theta, fudge_factor + np.sqrt(historical_grad))\n",
    "                self.theta = self.theta + stepsize * adj_grad\n",
    "\n",
    "                # save rmse and log-likelihood\n",
    "                if self.do_svgd and self.do_coin_svgd:\n",
    "                    self.svgd_rmses[iter], self.svgd_lls[iter], _, _ = self.evaluation(X_test, y_test)\n",
    "                elif self.do_svgd and not self.do_coin_svgd:\n",
    "                    self.svgd_rmses[iter], self.svgd_lls[iter] = self.evaluation(X_test, y_test)\n",
    "\n",
    "        # coin svgd\n",
    "        if self.do_coin_svgd:\n",
    "\n",
    "            # initialise gradient\n",
    "            grad_theta = np.zeros([self.M, num_vars])\n",
    "\n",
    "            # initialise other variables\n",
    "            L = self.L_init\n",
    "            grad_theta_sum = 0\n",
    "            abs_grad_theta_sum = 0\n",
    "            reward = 0\n",
    "\n",
    "            for iter in range(max_iter):\n",
    "\n",
    "                # sub sample\n",
    "                batch = [i % N0 for i in range(iter * batch_size, (iter + 1) * batch_size)]\n",
    "                for i in range(self.M):\n",
    "                    w1, b1, w2, b2, log_gamma, log_lambda = self.unpack_weights(self.theta_coin[i, :])\n",
    "                    dw1, db1, dw2, db2, dlog_gamma, dlog_lambda = logp_gradient(X_train[batch, :], y_train[batch], w1, b1,\n",
    "                                                                              w2,\n",
    "                                                                              b2, log_gamma, log_lambda, N0)\n",
    "                    grad_theta[i, :] = self.pack_weights(dw1, db1, dw2, db2, dlog_gamma, dlog_lambda)\n",
    "\n",
    "                # compute svgd gradient\n",
    "                kxy, dxkxy = self.svgd_kernel_coin(h=-1)\n",
    "\n",
    "                # gradient\n",
    "                grad_theta = (np.matmul(kxy, grad_theta) + dxkxy) / self.M\n",
    "\n",
    "                # | gradient |\n",
    "                abs_grad_theta = abs(grad_theta)\n",
    "\n",
    "                # max gradient\n",
    "                L = np.maximum(L, abs_grad_theta)\n",
    "\n",
    "                # sum of gradients\n",
    "                grad_theta_sum += grad_theta\n",
    "                abs_grad_theta_sum += abs_grad_theta\n",
    "\n",
    "                # 'reward'\n",
    "                reward = np.maximum(reward + np.multiply(self.theta_coin - self.theta_coin_init, grad_theta), 0)\n",
    "\n",
    "                self.theta_coin = self.theta_coin_init + grad_theta_sum / (L * np.maximum(abs_grad_theta_sum + L, self.alpha * L)) * (L + reward)\n",
    "\n",
    "                # record rmse and log-lik\n",
    "                if self.do_svgd and self.do_coin_svgd:\n",
    "                    _, _, self.coin_rmses[iter], self.coin_lls[iter] = self.evaluation(X_test, y_test)\n",
    "                elif self.do_coin_svgd:\n",
    "                    self.coin_rmses[iter], self.coin_lls[iter] = self.evaluation(X_test, y_test)\n",
    "\n",
    "        # model selection (svgd)\n",
    "        if self.do_svgd:\n",
    "            X_dev = self.normalization(X_dev)\n",
    "            for i in range(self.M):\n",
    "                w1, b1, w2, b2, log_gamma, log_lambda = self.unpack_weights(self.theta[i, :])\n",
    "                pred_y_dev = self.nn_predict(X_dev, w1, b1, w2, b2) * self.std_y_train + self.mean_y_train\n",
    "\n",
    "                # likelihood\n",
    "                def f_log_lik(log_gamma):\n",
    "                    return np.sum(np.log(np.sqrt(np.exp(log_gamma)) / np.sqrt(2 * np.pi) * np.exp(\n",
    "                        -1 * (np.power(pred_y_dev - y_dev, 2) / 2) * np.exp(log_gamma))))\n",
    "\n",
    "                lik1 = f_log_lik(log_gamma)\n",
    "\n",
    "                # heuristic\n",
    "                log_gamma = -np.log(np.mean(np.power(pred_y_dev - y_dev, 2)))\n",
    "                lik2 = f_log_lik(log_gamma)\n",
    "\n",
    "                # update log_gamma\n",
    "                if lik2 > lik1:\n",
    "                    self.theta[i, -2] = log_gamma\n",
    "\n",
    "        # model selection (coin svgd)\n",
    "        if self.do_coin_svgd:\n",
    "            X_dev_copy = self.normalization(X_dev_copy)\n",
    "            for i in range(self.M):\n",
    "                w1, b1, w2, b2, log_gamma, log_lambda = self.unpack_weights(self.theta_coin[i, :])\n",
    "                pred_y_dev_copy = self.nn_predict(X_dev_copy, w1, b1, w2, b2) * self.std_y_train + self.mean_y_train\n",
    "\n",
    "                # likelihood\n",
    "                def f_log_lik_copy(log_gamma):\n",
    "                    return np.sum(np.log(np.sqrt(np.exp(log_gamma)) / np.sqrt(2 * np.pi) * np.exp(\n",
    "                        -1 * (np.power(pred_y_dev_copy - y_dev_copy, 2) / 2) * np.exp(log_gamma))))\n",
    "\n",
    "                lik1 = f_log_lik_copy(log_gamma)\n",
    "\n",
    "                # heuristic\n",
    "                log_gamma = -np.log(np.mean(np.power(pred_y_dev_copy - y_dev_copy, 2)))\n",
    "                lik2 = f_log_lik_copy(log_gamma)\n",
    "\n",
    "                # update log_gamma\n",
    "                if lik2 > lik1:\n",
    "                    self.theta_coin[i, -2] = log_gamma\n",
    "\n",
    "    # normalisation\n",
    "    def normalization(self, X, y=None):\n",
    "        X = (X - np.full(X.shape, self.mean_X_train)) / np.full(X.shape, self.std_X_train)\n",
    "\n",
    "        if y is not None:\n",
    "            y = (y - self.mean_y_train) / self.std_y_train\n",
    "            return (X, y)\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    # initialisation\n",
    "    def init_weights(self, a0, b0):\n",
    "        w1 = 1.0 / np.sqrt(self.d + 1) * np.random.randn(self.d, self.n_hidden)\n",
    "        b1 = np.zeros((self.n_hidden,))\n",
    "        w2 = 1.0 / np.sqrt(self.n_hidden + 1) * np.random.randn(self.n_hidden)\n",
    "        b2 = 0.\n",
    "        log_gamma = np.log(np.random.gamma(a0, b0))\n",
    "        log_lambda = np.log(np.random.gamma(a0, b0))\n",
    "        return w1, b1, w2, b2, log_gamma, log_lambda\n",
    "\n",
    "    # svgd kernel\n",
    "    def svgd_kernel(self, h=-1):\n",
    "        sq_dist = pdist(self.theta)\n",
    "        pairwise_dists = squareform(sq_dist) ** 2\n",
    "\n",
    "        # median rule\n",
    "        if h < 0:\n",
    "            h = np.median(pairwise_dists)\n",
    "            h = np.sqrt(0.5 * h / np.log(self.theta.shape[0] + 1))\n",
    "\n",
    "        # rbf kernel\n",
    "        kxy = np.exp(-pairwise_dists / h ** 2 / 2)\n",
    "\n",
    "        # rbf kernel grad\n",
    "        dxkxy = -np.matmul(kxy, self.theta)\n",
    "        sumkxy = np.sum(kxy, axis=1)\n",
    "\n",
    "        for i in range(self.theta.shape[1]):\n",
    "            dxkxy[:, i] = dxkxy[:, i] + np.multiply(self.theta[:, i], sumkxy)\n",
    "\n",
    "        dxkxy = dxkxy / (h ** 2)\n",
    "\n",
    "        return kxy, dxkxy\n",
    "\n",
    "    # svgd kernel (for coin update)\n",
    "    def svgd_kernel_coin(self, h=-1):\n",
    "        sq_dist = pdist(self.theta_coin)\n",
    "        pairwise_dists = squareform(sq_dist) ** 2\n",
    "\n",
    "        # median rule\n",
    "        if h < 0:\n",
    "            h = np.median(pairwise_dists)\n",
    "            h = np.sqrt(0.5 * h / np.log(self.theta_coin.shape[0] + 1))\n",
    "\n",
    "        # rbf kernel\n",
    "        kxy = np.exp(-pairwise_dists / h ** 2 / 2)\n",
    "\n",
    "        # rbf kernel grad\n",
    "        dxkxy = -np.matmul(kxy, self.theta_coin)\n",
    "        sumkxy = np.sum(kxy, axis=1)\n",
    "\n",
    "        for i in range(self.theta_coin.shape[1]):\n",
    "            dxkxy[:, i] = dxkxy[:, i] + np.multiply(self.theta_coin[:, i], sumkxy)\n",
    "\n",
    "        dxkxy = dxkxy / (h ** 2)\n",
    "        return kxy, dxkxy\n",
    "\n",
    "    # pack parameters\n",
    "    def pack_weights(self, w1, b1, w2, b2, log_gamma, log_lambda):\n",
    "        params = np.concatenate([w1.flatten(), b1, w2, [b2], [log_gamma], [log_lambda]])\n",
    "        return params\n",
    "\n",
    "    # unpack parameters\n",
    "    def unpack_weights(self, z):\n",
    "        w = z\n",
    "        w1 = np.reshape(w[:self.d * self.n_hidden], [self.d, self.n_hidden])\n",
    "        b1 = w[self.d * self.n_hidden:(self.d + 1) * self.n_hidden]\n",
    "\n",
    "        w = w[(self.d + 1) * self.n_hidden:]\n",
    "        w2, b2 = w[:self.n_hidden], w[-3]\n",
    "\n",
    "        # the last two parameters are log variance\n",
    "        log_gamma, log_lambda = w[-2], w[-1]\n",
    "\n",
    "        return w1, b1, w2, b2, log_gamma, log_lambda\n",
    "\n",
    "    # evaluate test RMSE and log-likelihood\n",
    "    def evaluation(self, X_test, y_test):\n",
    "\n",
    "        # normalise\n",
    "        X_test = self.normalization(X_test)\n",
    "\n",
    "        # average over the output\n",
    "        pred_y_test = np.zeros([self.M, len(y_test)])\n",
    "        prob = np.zeros([self.M, len(y_test)])\n",
    "\n",
    "        pred_y_test_coin = np.zeros([self.M, len(y_test)])\n",
    "        prob_coin = np.zeros([self.M, len(y_test)])\n",
    "\n",
    "        if self.do_svgd:\n",
    "            for i in range(self.M):\n",
    "                w1, b1, w2, b2, log_gamma, log_lambda = self.unpack_weights(self.theta[i, :])\n",
    "                pred_y_test[i, :] = self.nn_predict(X_test, w1, b1, w2, b2) * self.std_y_train + self.mean_y_train\n",
    "                prob[i, :] = np.sqrt(np.exp(log_gamma)) / np.sqrt(2 * np.pi) * np.exp(\n",
    "                    -1 * (np.power(pred_y_test[i, :] - y_test, 2) / 2) * np.exp(log_gamma))\n",
    "            pred = np.mean(pred_y_test, axis=0)\n",
    "            svgd_rmse = np.sqrt(np.mean((pred - y_test) ** 2))\n",
    "            svgd_ll = np.mean(np.log(np.mean(prob, axis=0)))\n",
    "\n",
    "        if self.do_coin_svgd:\n",
    "            for i in range(self.M):\n",
    "                w1, b1, w2, b2, log_gamma, log_lambda = self.unpack_weights(self.theta_coin[i, :])\n",
    "                pred_y_test_coin[i, :] = self.nn_predict(X_test, w1, b1, w2, b2) * self.std_y_train + self.mean_y_train\n",
    "                prob_coin[i, :] = np.sqrt(np.exp(log_gamma)) / np.sqrt(2 * np.pi) * np.exp(\n",
    "                    -1 * (np.power(pred_y_test_coin[i, :] - y_test, 2) / 2) * np.exp(log_gamma))\n",
    "            pred_coin = np.mean(pred_y_test_coin, axis=0)\n",
    "            svgd_coin_rmse = np.sqrt(np.mean((pred_coin - y_test) ** 2))\n",
    "            svgd_coin_ll = np.mean(np.log(np.mean(prob_coin, axis=0)))\n",
    "\n",
    "        if self.do_svgd is True and self.do_coin_svgd is not True:\n",
    "            return svgd_rmse, svgd_ll\n",
    "        if self.do_svgd is not True and self.do_coin_svgd is True:\n",
    "            return svgd_coin_rmse, svgd_coin_ll\n",
    "        if self.do_svgd is True and self.do_coin_svgd is True:\n",
    "            return svgd_rmse, svgd_ll, svgd_coin_rmse, svgd_coin_ll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3289ea0",
   "metadata": {},
   "source": [
    "Now we're ready to run our method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaf3301",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# data sets\n",
    "data_names = [\"concrete\", \"energy\", \"protein-structure\", \"boston\", \"kin8nm\", \"wine\", \"yacht\"]\n",
    "data_names_abrv = [\"concrete\", \"energy\", \"protein\", \"boston\", \"kin8nm\", \"wine\", \"yacht\"]\n",
    "\n",
    "# number of repeats\n",
    "n_reps = 10\n",
    "\n",
    "# array to store times\n",
    "svgd_times = np.zeros((n_reps, (len(data_names))))\n",
    "coin_times = np.zeros((n_reps, (len(data_names))))\n",
    "\n",
    "for ii, data_name in enumerate(data_names):\n",
    "    data = np.loadtxt(open(\"data/\" + data_name + \".csv\", \"rb\"), delimiter=\",\", skiprows=1)\n",
    "\n",
    "    # last column is target\n",
    "    X_input = data[:, range(data.shape[1] - 1)]\n",
    "    y_input = data[:, data.shape[1] - 1]\n",
    "\n",
    "    step_sizes = np.logspace(-10, -0.5, 20)\n",
    "    n_sims = len(step_sizes)\n",
    "\n",
    "    svgd_results = np.zeros([n_reps, 2, n_sims])\n",
    "    svgd_coin_results = np.zeros([n_reps, 2])\n",
    "    svgd_coin_results_alt = np.zeros([n_reps, 2])\n",
    "\n",
    "    for rep in range(n_reps):\n",
    "\n",
    "        np.random.seed(rep)\n",
    "\n",
    "        # train and test data\n",
    "        train_ratio = 0.9\n",
    "        permutation = np.arange(X_input.shape[0])\n",
    "        random.shuffle(permutation)\n",
    "\n",
    "        size_train = int(np.round(X_input.shape[0] * train_ratio))\n",
    "        index_train = permutation[0: size_train]\n",
    "        index_test = permutation[size_train:]\n",
    "\n",
    "        X_train, y_train = X_input[index_train, :], y_input[index_train]\n",
    "        X_test, y_test = X_input[index_test, :], y_input[index_test]\n",
    "\n",
    "        # neural network parameters\n",
    "        batch_size, n_hidden, max_iter = 100, 50, 2000\n",
    "        alpha = 100\n",
    "\n",
    "        if data_name == \"protein-structure\":\n",
    "            n_hidden = 100\n",
    "\n",
    "        if data_name == \"boston\":\n",
    "            alpha = 1000\n",
    "\n",
    "        for id, step_size in enumerate(step_sizes):\n",
    "            print(\"Dataset: \" + str(data_name) + \", Repetition: \" + str(rep) + \"/\" + str(n_reps) + \", Iteration \" + str(id) + \"/\" + str(len(step_sizes)))\n",
    "            if id == 0:\n",
    "                svgd_start = time.time()\n",
    "                svgd = BayesNN(X_train, y_train, X_test, y_test, batch_size=batch_size, n_hidden=n_hidden,\n",
    "                               max_iter=max_iter, stepsize=step_size, do_svgd=True, do_coin_svgd=False,\n",
    "                               alpha=alpha)\n",
    "                svgd_end = time.time()\n",
    "                svgd_time = svgd_end - svgd_start\n",
    "                svgd_rmse, svgd_ll = svgd.evaluation(X_test, y_test)\n",
    "\n",
    "                coin_start = time.time()\n",
    "                svgd_coin = BayesNN(X_train, y_train, X_test, y_test, batch_size=batch_size, n_hidden=n_hidden,\n",
    "                                    max_iter=max_iter, stepsize=step_size, do_svgd=False, do_coin_svgd=True,\n",
    "                                    alpha=alpha)\n",
    "                coin_end = time.time()\n",
    "                coin_time = coin_end - coin_start\n",
    "                svgd_coin_rmse, svgd_coin_ll = svgd_coin.evaluation(X_test, y_test)\n",
    "\n",
    "            if id > 0:\n",
    "                svgd = BayesNN(X_train, y_train, X_test, y_test, batch_size=batch_size, n_hidden=n_hidden,\n",
    "                                    max_iter=max_iter, stepsize=step_size, do_coin_svgd=False)\n",
    "                svgd_rmse, svgd_ll = svgd.evaluation(X_test, y_test)\n",
    "\n",
    "            svgd_times[rep, ii] = svgd_time\n",
    "            coin_times[rep, ii] = coin_time\n",
    "\n",
    "            svgd_results[rep, 0, id], svgd_results[rep, 1, id] = svgd_rmse, svgd_ll\n",
    "            svgd_coin_results[rep, 0], svgd_coin_results[rep, 1] = svgd_coin_rmse, svgd_coin_ll\n",
    "\n",
    "    np.save(results_dir + \"/\" + \"svgd_\" + data_name, svgd_results)\n",
    "    np.save(results_dir + \"/\" + \"coin_svgd_\" + data_name, svgd_coin_results)\n",
    "\n",
    "    # plot rmse vs learning rate\n",
    "    plt.close(\"all\")\n",
    "    mean_svgd = np.mean(svgd_results, axis=0)\n",
    "    upper_svgd, lower_svgd = return_confidence_interval(svgd_results)\n",
    "    mean_coin = np.mean(svgd_coin_results, axis=0)\n",
    "    upper_coin, lower_coin = return_confidence_interval(svgd_coin_results)\n",
    "    mean_coin_alt = np.mean(svgd_coin_results_alt, axis=0)\n",
    "    upper_coin_alt, lower_coin_alt = return_confidence_interval(svgd_coin_results_alt)\n",
    "\n",
    "    plt.plot(step_sizes[:-1], mean_svgd[0,:-1], \".-\", label=\"SVGD\", color=\"C0\")\n",
    "    plt.fill_between(step_sizes[:-1], lower_svgd[0,:-1], upper_svgd[0,:-1], color=\"C0\", alpha=0.1)\n",
    "    plt.grid(visible=True, color=\"whitesmoke\", ls='-')\n",
    "    plt.axhline(y=mean_coin[0], xmin=0, xmax=1, color=\"C1\", label=\"Coin SVGD\")\n",
    "    plt.fill_between(step_sizes[:-1], lower_coin[0], upper_coin[0], color=\"C1\", alpha=0.1)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlabel(\"Learning Rate\", fontsize=18)\n",
    "    plt.ylabel(\"Test RMSE\", fontsize=18)\n",
    "    plt.legend(prop={'size': 15})\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.gcf().subplots_adjust(bottom=0.15, left=0.15)\n",
    "    if data_name == \"kin8nm\":\n",
    "        plt.gcf().subplots_adjust(bottom=0.15, left=0.20)\n",
    "    fname = plot_dir + \"/\" + \"rmse_vs_lr_\" + data_name + \".pdf\"\n",
    "    plt.savefig(fname, format=\"pdf\")\n",
    "    plt.show()\n",
    "\n",
    "    # plot log-lik vs learning rate\n",
    "    plt.plot(step_sizes, mean_svgd[1,:], \".-\", label=\"SVGD\")\n",
    "    plt.fill_between(step_sizes, lower_svgd[1,:], upper_svgd[1,:], color=\"C0\", alpha=0.1)\n",
    "    plt.grid(visible=True, color=\"whitesmoke\", ls='-')\n",
    "    plt.axhline(y=mean_coin[1], xmin=0, xmax=1, color=\"C1\", label=\"Coin SVGD\")\n",
    "    plt.fill_between(step_sizes, lower_coin[1], upper_coin[1], color=\"C1\", alpha=0.1)\n",
    "    plt.legend()\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlabel(\"Learning Rate\", fontsize=18)\n",
    "    plt.ylabel(\"Test Log-Likelihood\", fontsize=18)\n",
    "    plt.legend(prop={'size': 15})\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.gcf().subplots_adjust(bottom=0.15, left=0.18)\n",
    "    if data_name == \"kin8nm\":\n",
    "        plt.gcf().subplots_adjust(bottom=0.15, left=0.20)\n",
    "    fname = plot_dir + \"/\" + \"ll_vs_lr_\" + data_name + \".pdf\"\n",
    "    plt.savefig(fname, format=\"pdf\")\n",
    "    plt.show()\n",
    "\n",
    "np.save(results_dir + \"/\" + \"all_svgd_times_\" + data_name, svgd_times)\n",
    "np.save(results_dir + \"/\" + \"all_coin_times_\" + data_name, coin_times)\n",
    "\n",
    "# times\n",
    "mean_svgd_time = np.mean(svgd_times, axis=0)\n",
    "lower_svgd_time, upper_svgd_time = return_confidence_interval(svgd_times)\n",
    "lower_svgd_time[-1], upper_svgd_time[-1] = mean_svgd_time[-1]-1e-10, mean_svgd_time[-1]+1e-10\n",
    "mean_svgd_time_CIs = [mean_svgd_time - lower_svgd_time, upper_svgd_time - mean_svgd_time]\n",
    "\n",
    "mean_coin_time = np.mean(coin_times, axis=0)\n",
    "lower_coin_time, upper_coin_time = return_confidence_interval(coin_times)\n",
    "lower_coin_time[-1],  upper_coin_time[-1] = mean_coin_time[-1], mean_coin_time[-1]\n",
    "mean_coin_time_CIs = [mean_coin_time - lower_coin_time, upper_coin_time - mean_coin_time]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(range(8), mean_svgd_time, marker=\"o\", color=\"C0\", label=\"SVGD\", s=100, zorder=10)\n",
    "plt.scatter([x+0.2 for x in range(8)], mean_coin_time, marker=\"D\", color=\"C1\", label=\"Coin SVGD\", s=100, zorder=9)\n",
    "plt.legend(prop={'size': 18})\n",
    "plt.grid(color='whitesmoke')\n",
    "plt.xlabel(\"Dataset\", fontsize=18)\n",
    "plt.ylabel(\"Clock Time (s)\", fontsize=18)\n",
    "x_tick_locs = np.arange(len(data_names_abrv))\n",
    "plt.xticks(x_tick_locs, data_names_abrv, rotation=60)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "fname = plot_dir + \"/\" + \"all_times\" + \".pdf\"\n",
    "plt.savefig(fname, bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_m1",
   "language": "python",
   "name": "venv_m1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
