{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0dd2cef",
   "metadata": {},
   "source": [
    "## Bayesian Probabilistic Matrix Factorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ca6520",
   "metadata": {},
   "source": [
    "In this notebook, we'll use Coin SVGD for Bayesian probablistic matrix factorisation (PMF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efc0d610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, random\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "991565fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dir = \"plots/SVGD/BayesPMF\"\n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)\n",
    "\n",
    "results_dir = \"results/SVGD/BayesPMF\"\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae71b450",
   "metadata": {},
   "source": [
    "### Model\n",
    "Now we're ready to define our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd702539",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Probabilistic Matrix Factorisation\n",
    "\"\"\"\n",
    "\n",
    "class BayesianPMF:\n",
    "\n",
    "    def __init__(self, data, batchsize=100, alpha=3, mu0=0, a0=4, b0=5, D=20, N=943, M=1682):\n",
    "        \"\"\"\n",
    "        :param data: input data\n",
    "        :param batchsize: batchsize for stochastic gradients\n",
    "        :param alpha: precision of observation noise (hyper-parameter)\n",
    "        :param mu0: mean of mu_W (hyper-parameter)\n",
    "        :param a0: location parameter of lambda_W (hyper-parameter)\n",
    "        :param b0: scale parameter of lambda_W (hyper-parameter)\n",
    "        :param D: latent dimension\n",
    "        :param N: number of users\n",
    "        :param M: number of movies\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.totalsize, _ = data.shape\n",
    "        self.batchsize = batchsize\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.mu0 = mu0\n",
    "        self.a0 = a0\n",
    "        self.b0 = b0\n",
    "\n",
    "        self.D = D\n",
    "        self.N = N\n",
    "        self.M = M\n",
    "\n",
    "        self.iter = 0\n",
    "        self.permutation = np.random.permutation(self.totalsize)\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def log_lik(self, params, data):\n",
    "        \"\"\"\n",
    "        log likelihood for a single data point\n",
    "        \"\"\"\n",
    "        U, V, _, _, _, _ =  params\n",
    "        i, j, r_ij = data\n",
    "        return (-self.alpha/2)*(r_ij - jnp.dot(U[:,(i-1).astype(int)].T, V[:, (j-1).astype(int)]))**2\n",
    "\n",
    "    def batch_log_lik(self, params, data):\n",
    "        \"\"\"\n",
    "        log likelihood for batch of data\n",
    "        \"\"\"\n",
    "        return vmap(self.log_lik, in_axes=(None, 0))(params, data)\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def log_pri_m(self, M, mu_M, lambda_M):\n",
    "        \"\"\"\n",
    "        log prior for single vectors U_i and V_j\n",
    "        \"\"\"\n",
    "        return -0.5*jnp.linalg.multi_dot([M-mu_M, jnp.diag(jnp.exp(lambda_M)), M-mu_M]) + 0.5*jnp.sum(lambda_M)\n",
    "\n",
    "    def batch_log_pri_m(self, M, mu_M, lambda_M):\n",
    "        \"\"\"\n",
    "        log prior for batch of vectors U_i and V_j\n",
    "        \"\"\"\n",
    "        return jit(vmap(self.log_pri_m, in_axes=(1, None, None)))(M, mu_M, lambda_M)\n",
    "\n",
    "    def log_pri_mu(self, mu_M, lambda_M):\n",
    "        \"\"\"\n",
    "        log prior of mu\n",
    "        \"\"\"\n",
    "        return -0.5*jnp.linalg.multi_dot([mu_M, jnp.diag(jnp.exp(lambda_M)), mu_M]) + 0.5*jnp.sum(lambda_M)\n",
    "\n",
    "    def log_pri_lambda(self, log_lambda_i):\n",
    "        \"\"\"\n",
    "        log prior of lambda (precision)\n",
    "        \"\"\"\n",
    "        return (self.a0-1)*log_lambda_i - self.b0*jnp.exp(log_lambda_i) + log_lambda_i\n",
    "\n",
    "    def batch_log_pri_lambda(self, log_lambda_i):\n",
    "        \"\"\"\n",
    "        batch log prior of lambda\n",
    "        \"\"\"\n",
    "        return vmap(self.log_pri_lambda, in_axes=(0))(log_lambda_i)\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def log_prior(self, params):\n",
    "        \"\"\"\n",
    "        Prior for U, V, mu_U, lambda_U, mu_V, lambda_V\n",
    "        \"\"\"\n",
    "        U, V, mu_U, lambda_U, mu_V, lambda_V = params\n",
    "\n",
    "        term_U = jnp.sum(self.batch_log_pri_m(U, mu_U, lambda_U))\n",
    "        term_V = jnp.sum(self.batch_log_pri_m(V, mu_V, lambda_V))\n",
    "\n",
    "        term_muU = self.log_pri_mu(mu_U, lambda_U)\n",
    "        term_muV = self.log_pri_mu(mu_V, lambda_V)\n",
    "\n",
    "        term_lambdaU = jnp.sum(self.batch_log_pri_lambda(lambda_U))\n",
    "        term_lambdaV = jnp.sum(self.batch_log_pri_lambda(lambda_V))\n",
    "\n",
    "        return term_U + term_V + term_muU + term_muV + term_lambdaU + term_lambdaV\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def log_post(self, params, data):\n",
    "        Ndata = 80000 # dataset size\n",
    "        return self.log_prior(params) + Ndata*jnp.mean(self.batch_log_lik(params, data))\n",
    "\n",
    "    def log_post_flat(self, params, data):\n",
    "        \"\"\"\n",
    "        Log posterior density\n",
    "        \"\"\"\n",
    "        params = self.unpack_params(params)\n",
    "        return self.log_prior(params) + self.totalsize * jnp.mean(self.batch_log_lik(params, data))\n",
    "\n",
    "    def pack_params(self, params):\n",
    "        \"\"\"\n",
    "        Pack all parameters\n",
    "        \"\"\"\n",
    "        return jnp.concatenate([x.ravel() for x in params])\n",
    "\n",
    "    def unpack_params(self, params):\n",
    "        \"\"\"\n",
    "        Unpack all parameters\n",
    "        \"\"\"\n",
    "        dims = jnp.array([self.D * self.N, self.D * self.M, self.D, self.D, self.D, self.D])\n",
    "        breaks = jnp.cumsum(dims)\n",
    "        U = jnp.reshape(params[0:breaks[0]], (self.D, self.N))\n",
    "        V = jnp.reshape(params[breaks[0]:breaks[1]], (self.D, self.M))\n",
    "        mu_U = params[breaks[1]:breaks[2]]\n",
    "        lambda_U = params[breaks[2]:breaks[3]]\n",
    "        mu_V = params[breaks[3]:breaks[4]]\n",
    "        lambda_V = params[breaks[4]:breaks[5]]\n",
    "        return [U, V, mu_U, lambda_U, mu_V, lambda_V]\n",
    "\n",
    "    def predict_loss(self, U, V, data, mean_rating):\n",
    "        \"\"\"\n",
    "        Predictive RMSE for a single data point and a single sample\n",
    "        \"\"\"\n",
    "        i, j, r_ij = data\n",
    "        pred = jnp.dot(U[:, (i - 1).astype(int)].T, V[:, (j - 1).astype(int)]) + mean_rating\n",
    "        pred = jnp.where(pred > 5, 5, pred)\n",
    "        pred = jnp.where(pred < 1, 1, pred)\n",
    "        return (pred - r_ij) ** 2\n",
    "\n",
    "    def sgld(self, params0, dt=1e-5, n_iter=1000):\n",
    "        \"\"\"\n",
    "        Stochastic Gradient Langevin Dynamics (SGLD)\n",
    "        \"\"\"\n",
    "\n",
    "        # initial theta\n",
    "        params0 = self.pack_params(params0)\n",
    "        params = deepcopy(params0)\n",
    "\n",
    "        # grad log pdf\n",
    "        grad_log_pdf = grad(self.log_post_flat)\n",
    "\n",
    "        # noise\n",
    "        key = random.PRNGKey(1)\n",
    "        w = jnp.sqrt(2 * dt) * random.normal(key=key, shape=(n_iter,) + params.shape)\n",
    "\n",
    "        for t in tqdm(range(n_iter)):\n",
    "\n",
    "            if self.batchsize > 0:\n",
    "                batch = [i % self.totalsize for i in\n",
    "                         range(self.iter * self.batchsize, (self.iter + 1) * self.batchsize)]\n",
    "                ridx = self.permutation[batch]\n",
    "                self.iter += 1\n",
    "            else:\n",
    "                ridx = np.random.permutation(self.data.shape[0])\n",
    "\n",
    "            batch_data = self.data[ridx, :]\n",
    "\n",
    "            grad_theta = grad_log_pdf(params, batch_data)\n",
    "\n",
    "            params = params + dt * grad_theta + w[t, :]\n",
    "\n",
    "        return self.unpack_params(params)\n",
    "\n",
    "    def gram(self, kernel, xs):\n",
    "        return vmap(lambda x: vmap(lambda y: kernel(x, y))(xs))(xs)\n",
    "\n",
    "    def rbf(self, x, y):\n",
    "        return jnp.exp(-jnp.sum((x - y) ** 2))\n",
    "\n",
    "    def svgd_kernel(self, theta):\n",
    "        Kxy = self.gram(self.rbf, theta)\n",
    "        k_grad = grad(self.rbf)\n",
    "        return Kxy, k_grad(theta, theta)\n",
    "\n",
    "    def svgd(self, params0, dt=1e-5, n_iter=1000, adagrad=True, alpha=0.9):\n",
    "        \"\"\"\n",
    "        Stein Variational Gradient Descent\n",
    "        \"\"\"\n",
    "        # initial theta\n",
    "        params = deepcopy(params0)\n",
    "\n",
    "        # for adagrad with momentum\n",
    "        fudge_factor = 1e-6\n",
    "        historical_grad = 0\n",
    "\n",
    "        # grad log pdf\n",
    "        grad_log_pdf = grad(self.log_post_flat)\n",
    "\n",
    "        for t in tqdm(range(n_iter)):\n",
    "\n",
    "            if self.batchsize > 0:\n",
    "                batch = [i % self.totalsize for i in range(self.iter * self.batchsize, (self.iter + 1) * self.batchsize)]\n",
    "                ridx = self.permutation[batch]\n",
    "                self.iter += 1\n",
    "            else:\n",
    "                ridx = np.random.permutation(self.data.shape[0])\n",
    "\n",
    "            batch_data = self.data[ridx, :]\n",
    "\n",
    "            ln_p_grad = np.zeros_like(params)\n",
    "            for k in range(params.shape[0]):\n",
    "                ln_p_grad[k, :] = grad_log_pdf(params[k, :], batch_data)\n",
    "            kxy, dxkxy = self.svgd_kernel(params)\n",
    "            grad_params = (jnp.matmul(kxy, ln_p_grad) + dxkxy) / params.shape[0]\n",
    "\n",
    "            if adagrad:\n",
    "                if t == 0:\n",
    "                    historical_grad = historical_grad + grad_params ** 2\n",
    "                else:\n",
    "                    historical_grad = alpha * historical_grad + (1 - alpha) * (grad_params ** 2)\n",
    "\n",
    "                adj_grad = np.divide(grad_params, fudge_factor + np.sqrt(historical_grad))\n",
    "\n",
    "            else:\n",
    "                adj_grad = grad_params\n",
    "\n",
    "            params = params + dt * adj_grad\n",
    "\n",
    "        return params\n",
    "\n",
    "    def svgd_param_free(self, theta0, n_iter=1000):\n",
    "        \"\"\"\n",
    "        Coin Stein Variational Gradient Descent (Coin SVGD)\n",
    "        \"\"\"\n",
    "\n",
    "        # initial theta\n",
    "        theta = deepcopy(theta0)\n",
    "\n",
    "        # grad log pdf\n",
    "        grad_log_pdf = grad(self.log_post_flat)\n",
    "\n",
    "        # initialise other vars\n",
    "        L = 0\n",
    "        grad_theta_sum = 0\n",
    "        reward = 0\n",
    "        abs_grad_theta_sum = 0\n",
    "\n",
    "        for t in tqdm(range(n_iter)):\n",
    "\n",
    "            # data batch\n",
    "            if self.batchsize > 0:\n",
    "                batch = [i % self.totalsize for i in\n",
    "                         range(self.iter * self.batchsize, (self.iter + 1) * self.batchsize)]\n",
    "                ridx = self.permutation[batch]\n",
    "                self.iter += 1\n",
    "            else:\n",
    "                ridx = np.random.permutation(self.data.shape[0])\n",
    "\n",
    "            batch_data = self.data[ridx, :]\n",
    "\n",
    "            # calculate grad log density\n",
    "            ln_p_grad = np.zeros_like(theta)\n",
    "            for k in range(theta.shape[0]):\n",
    "                ln_p_grad[k, :] = grad_log_pdf(theta[k, :], batch_data)\n",
    "\n",
    "            # calculate kernel matrix\n",
    "            kxy, dx_kxy = self.svgd_kernel(theta)\n",
    "            grad_theta = (jnp.matmul(kxy, ln_p_grad) + dx_kxy) / theta0.shape[0]\n",
    "\n",
    "            # |gradient|\n",
    "            abs_grad_theta = abs(grad_theta)\n",
    "\n",
    "            # constant\n",
    "            L = jnp.maximum(abs_grad_theta, L)\n",
    "\n",
    "            # sum of gradients\n",
    "            grad_theta_sum += grad_theta\n",
    "            abs_grad_theta_sum += abs_grad_theta\n",
    "\n",
    "            # 'reward'\n",
    "            reward = np.maximum(reward + jnp.multiply(theta - theta0, grad_theta), 0)\n",
    "\n",
    "            # theta update\n",
    "            theta = theta0 + grad_theta_sum / (L * (abs_grad_theta_sum + L)) * (L + reward)\n",
    "\n",
    "        return theta\n",
    "\n",
    "    def _predict_loss(self, U, V, data, mean_rating):\n",
    "        \"\"\"\n",
    "        Get the predictive RMSE for a single data point and a single sample\n",
    "        \"\"\"\n",
    "        i, j, r_ij = data\n",
    "        pred = jnp.dot(U[:,(i-1).astype(int)].T, V[:, (j-1).astype(int)]) + mean_rating\n",
    "        pred = jnp.where(pred>5, 5, pred)\n",
    "        pred = jnp.where(pred<1, 1, pred)\n",
    "        return (pred-r_ij)**2\n",
    "\n",
    "    def rmse_1sample(self, params, test_data, mean_rating):\n",
    "        \"\"\"\n",
    "        RMSE for 1 sample for 1 sample Take average over all data\n",
    "        \"\"\"\n",
    "        batch_predict_loss = vmap(self._predict_loss, in_axes=(None, None, 0, None))\n",
    "        return jnp.mean(batch_predict_loss(params[0], params[1], test_data, mean_rating))\n",
    "\n",
    "    def rmse_batch(self, params, test_data, mean_rating):\n",
    "        \"\"\"\n",
    "        RMSE for all particles\n",
    "        \"\"\"\n",
    "        return sum([self.rmse_1sample(self.unpack_params(params[i,:]), test_data, mean_rating) for i in range(params.shape[0])]) / params.shape[0]\n",
    "\n",
    "    def init_params(self, key):\n",
    "        subkey1, subkey2, subkey3, subkey4, subkey5, subkey6 = random.split(key, 6)\n",
    "        U = random.normal(subkey1, shape=(self.D, self.N))\n",
    "        V = random.normal(subkey2, shape=(self.D, self.M))\n",
    "        mu_U = random.normal(subkey3, shape=(self.D,))\n",
    "        lambda_U = 0.1 * random.normal(subkey4, shape=(self.D,))\n",
    "        mu_V = random.normal(subkey5, shape=(self.D,))\n",
    "        lambda_V = 0.1 * random.normal(subkey6, shape=(self.D,))\n",
    "        params = [U, V, mu_U, lambda_U, mu_V, lambda_V]\n",
    "        return [0.1*p for p in params]\n",
    "\n",
    "    def batch_init_params(self, key, n_particles):\n",
    "        total_dim = self.D * (self.N + self.M + 1 + 1 + 1 + 1)\n",
    "        params = np.zeros((n_particles, total_dim))\n",
    "        for i in range(n_particles):\n",
    "            key, subkey = random.split(key)\n",
    "            params[i, :] = self.pack_params(self.init_params(subkey))\n",
    "        return jnp.array(params)\n",
    "\n",
    "    def init_PMF_zeros(self):\n",
    "        U = jnp.zeros(shape=(self.D, self.N))\n",
    "        V = jnp.zeros(shape=(self.D, self.M))\n",
    "        mu_U = jnp.zeros(shape=(self.D,))\n",
    "        lambda_U = jnp.zeros(shape=(self.D,))\n",
    "        mu_V = jnp.zeros(shape=(self.D,))\n",
    "        lambda_V = jnp.zeros(shape=(self.D,))\n",
    "        params = [U, V, mu_U, lambda_U, mu_V, lambda_V]\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5ac8d6",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "We can now run all of the methods. \n",
    "* We'll run all of the algorithms for T=1000 iterations, using N=50 particles. \n",
    "* For the learning-rate dependent methods (SVGD and SGLD), we'll consider a grid of candidate learning rates, and run them for each of theses. Note: running this grid search over learning rates can take some time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566041e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "R_train = np.genfromtxt(\"data/MovieLens/train.dat\")\n",
    "R_test = np.genfromtxt(\"data/MovieLens/test.dat\")\n",
    "\n",
    "mean_rating = jnp.mean(R_train[:,2])\n",
    "R_train[:, 2] = R_train[:, 2] - mean_rating\n",
    "R_train = jnp.array(R_train)\n",
    "\n",
    "# model parameters\n",
    "D = 20\n",
    "N = 943\n",
    "M = 1682\n",
    "\n",
    "alpha = 3\n",
    "mu0 = 0\n",
    "a0 = 1\n",
    "b0 = 5\n",
    "\n",
    "# algorithm parameters\n",
    "n_iter = 1000\n",
    "batchsize = 1000\n",
    "n_particles = 50\n",
    "\n",
    "all_key_int = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "all_dt = list(list(np.logspace(-7, 0, 30)))\n",
    "\n",
    "vars = [all_key_int, all_dt]\n",
    "all_vars = list(itertools.product(*vars))\n",
    "\n",
    "sgld_rmse = np.zeros((len(all_key_int), len(all_dt)))\n",
    "svgd_rmse = np.zeros((len(all_key_int), len(all_dt)))\n",
    "coin_svgd_rmse = np.zeros((len(all_key_int)))\n",
    "\n",
    "for ii, key_int in enumerate(all_key_int):\n",
    "\n",
    "    for jj, dt in enumerate(all_dt):\n",
    "\n",
    "        print(\"Key: \" + str(ii + 1) + \"/\" + str(len(all_key_int)))\n",
    "        print(\"LR:\" + str(jj + 1) + \"/\" + str(len(all_dt)))\n",
    "\n",
    "        key = random.PRNGKey(key_int)\n",
    "        PMF = BayesianPMF(data=R_train, batchsize=batchsize, alpha=alpha, mu0=mu0, a0=a0, b0=b0, D=D, N=N, M=M)\n",
    "\n",
    "        theta0_sgld = [p for p in PMF.init_params(key)]\n",
    "        theta_sgld = PMF.sgld(theta0_sgld, n_iter=n_iter, dt=dt)\n",
    "        init_rmse_sgld = PMF.rmse_1sample(theta0_sgld, R_test, mean_rating)\n",
    "        final_rmse_sgld = PMF.rmse_1sample(theta_sgld, R_test, mean_rating)\n",
    "        sgld_rmse[ii, jj] = final_rmse_sgld\n",
    "\n",
    "        theta0_svgd = PMF.batch_init_params(key, n_particles)\n",
    "        theta_svgd = PMF.svgd(theta0_svgd, n_iter=n_iter, dt=dt)\n",
    "        init_rmse_svgd = PMF.rmse_batch(theta0_svgd, R_test, mean_rating)\n",
    "        final_rmse_svgd = PMF.rmse_batch(theta_svgd, R_test, mean_rating)\n",
    "        svgd_rmse[ii,jj] = final_rmse_svgd\n",
    "        \n",
    "        if jj == 0:\n",
    "            theta0_coin_svgd = PMF.batch_init_params(key, n_particles)\n",
    "            theta_coin_svgd = PMF.svgd_param_free(theta0_coin_svgd, n_iter=n_iter)\n",
    "            init_rmse_coin_svgd = PMF.rmse_batch(theta0_coin_svgd, R_test, mean_rating)\n",
    "            final_rmse_coin_svgd = PMF.rmse_batch(theta_coin_svgd, R_test, mean_rating)\n",
    "            coin_svgd_rmse[ii] = final_rmse_coin_svgd\n",
    "\n",
    "np.save(results_dir + \"svgd\", svgd_rmse)\n",
    "np.save(results_dir + \"coin_svgd\", coin_svgd_rmse)\n",
    "np.save(results_dir + \"sgld\", sgld_rmse)\n",
    "\n",
    "rmses = [svgd_rmse, coin_svgd_rmse, sgld_rmse]\n",
    "fnames = [\"svgd\", \"coin_svgd\", \"sgld\"]\n",
    "names = [\"SVGD\", \"Coin SVGD\", \"SGLD\"]\n",
    "\n",
    "# average over random seeds\n",
    "average_rmses = [np.mean(rmse, 0) for rmse in rmses]\n",
    "\n",
    "# plot results\n",
    "for ii, (rmse, average_rmse, name, fname) in enumerate(zip(rmses, average_rmses, names, fnames)):\n",
    "    if name == \"SGLD\" or name == \"SVGD\":\n",
    "        plt.fill_between(all_dt, np.amax(rmse, 0), np.amin(rmse, 0), color=\"C\" + str(ii), alpha=0.4)\n",
    "        plt.plot(all_dt, average_rmse, \".-\", label=name, color=\"C\" + str(ii), alpha=1)\n",
    "    if name == \"Coin SVGD\":\n",
    "        plt.axhline(average_rmse, label=name, color=\"C\" + str(ii))\n",
    "        xlim = plt.gca().get_xlim()\n",
    "        for jj in range(len(all_key_int)):\n",
    "            plt.axhline(rmse[jj], color=\"C\" + str(ii), alpha=0.3)\n",
    "        plt.gca().get_xlim()\n",
    "plt.xscale(\"log\")\n",
    "plt.grid(visible=True, color=\"whitesmoke\", ls='-')\n",
    "plt.gca().set_axisbelow(True)\n",
    "plt.ylim(0.8, 1.4)\n",
    "plt.legend(prop={'size':18})\n",
    "plt.xlabel(\"Learning Rate\", fontsize=18)\n",
    "plt.ylabel(\"Test RMSE\", fontsize=18)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.gcf().subplots_adjust(bottom=0.15)\n",
    "fname = plot_dir + \"/\" + \"lr_vs_rmse_\" + \".pdf\"\n",
    "plt.savefig(fname, format=\"pdf\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_m1",
   "language": "python",
   "name": "venv_m1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
